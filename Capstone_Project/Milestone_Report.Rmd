
---
title: "Exploratory Data Analysis for Natural Language Processing"
output: 
     html_document:
          pandoc_args: [
               "+RTS", "-k64m",
               "-RTS"
          ]
---

``` {r global_options, include=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, error=FALSE)
```

``` {r libraries, include=FALSE}
library(plyr)
library(tm)
library(RWeka)
library(pryr)
library(ggplot2)
```

``` {r read_in_full_data, include=FALSE, cache=TRUE}
# Read in and tokenize the datasets
source('~/Documents/School/Johns Hopkins Data Science/datasciencecoursera/Capstone_Project/clean_and_filter.R')

rawFiles = c("en_US.twitter.txt", "en_US.news.txt", "en_US.blogs.txt")
fileDir <- "~/Documents/School/Coursera Data Science/Capstone Project/final/en_US/"
rawFileList <- list()
rawNcharList <- list()
for (i in rawFiles){
     filePath <- paste0(fileDir, i)
     # create a connection for the file
     con <- file(filePath, open="r")
     
     # read in the lines and close the connection
     rawFileList[[i]] <- readLines(con) 
     close(con)
}
```

``` {r clean_and_filter_data, include=FALSE, cache=TRUE}
US.path <- paste0(fileDir,"sampled/en_US.txt")
cleanedCombined <- clean_and_filter(US.path)
Blogs.path <- paste0(fileDir,"sampled/en_US.blogs.txt")
cleanedBlogs <- clean_and_filter(Blogs.path)
News.path <- paste0(fileDir,"sampled/en_US.news.txt")
cleanedNews <- clean_and_filter(News.path)
Twitter.path <- paste0(fileDir,"sampled/en_US.twitter.txt")
cleanTwitter <- clean_and_filter(Twitter.path)
```

``` {r tokenize_combined_data, include=FALSE, cache=TRUE}
source('~/Documents/School/Johns Hopkins Data Science/datasciencecoursera/Capstone_Project/tokenize_file.R')
tokenCombined <- Tokenize(cleanedCombined)
```

``` {r tokenize_blogs_data, include=FALSE, cache=TRUE}
source('~/Documents/School/Johns Hopkins Data Science/datasciencecoursera/Capstone_Project/tokenize_file.R')
tokenBlogs <- Tokenize(cleanedBlogs)
```

``` {r tokenize_news_data, include=FALSE, cache=TRUE}
source('~/Documents/School/Johns Hopkins Data Science/datasciencecoursera/Capstone_Project/tokenize_file.R')
tokenNews <- Tokenize(cleanedNews)
```

``` {r tokenize_twitter_data, include=FALSE, cache=TRUE}
source('~/Documents/School/Johns Hopkins Data Science/datasciencecoursera/Capstone_Project/tokenize_file.R')
tokenTwitter <- Tokenize(cleanTwitter)
```

``` {r summary_sampled_data, include=FALSE, cache=TRUE}
# Create summary data tables for each type of n-gram
FileList <- c("Blogs", "News", "Twitter", "Combined")
WordVals <- c(tokenBlogs$NGramCounts$Total.Count[1],
                 tokenNews$NGramCounts$Total.Count[1], 
               tokenTwitter$NGramCounts$Total.Count[1],
               tokenCombined$NGramCounts$Total.Count[1])
TwoGramVals <- c(tokenBlogs$NGramCounts$Total.Count[2],
               tokenNews$NGramCounts$Total.Count[2], 
               tokenTwitter$NGramCounts$Total.Count[2],
               tokenCombined$NGramCounts$Total.Count[2])
ThreeGramVals <- c(tokenBlogs$NGramCounts$Total.Count[3],
               tokenNews$NGramCounts$Total.Count[3], 
               tokenTwitter$NGramCounts$Total.Count[3],
               tokenCombined$NGramCounts$Total.Count[3])
FourGramVals <- c(tokenBlogs$NGramCounts$Total.Count[4],
               tokenNews$NGramCounts$Total.Count[4], 
               tokenTwitter$NGramCounts$Total.Count[4],
               tokenCombined$NGramCounts$Total.Count[4])
GramSummary <- data.frame(FileList, WordVals, TwoGramVals, ThreeGramVals,
                           FourGramVals)
colnames(GramSummary) <- c("Dataset", "Total.Words", "Total.Two.Grams",
                            "Total.Three.Grams", "Total.Four.Grams")
```

# Introduction
The purpose of this report is to detail initial exploratory data analysis of a corpus of English text obtained from some Twitter feeds, blog posts, and news articles.  We will analyze the makeup of the source corpora, and we will describe the tokenization of (smaller) sampled data sets from the original corpus.

# Exploratory Data Analysis
The corpus is set of large text files consisting of lines of text separated by carriage return, or newline, characters.  The size of the data set is

``` {r, cache=TRUE}
for (i in 1:length(rawFiles)) {
     print(paste0("File ", names(rawFileList[i]), " has size ",
           object_size(rawFileList[i]), " bytes containing ",
           length(rawFileList[[i]]), " lines of text."))
}
```

Since this data set is so large, we will not count the number of words, or n-grams.  Instead, we will generate a smaller data set using binomial sampling with p=.025 (2.5%), followed by combining the three sampled files into one file.  Given the size of the original data set, this sampled data set adequately represents the population.  Thus, our exploratory data analysis will consist of a deep dive into the sampled data.

## Tokenization of the Sampled Dataset
Tokenization of the data set consists of the following steps:

1. Replace profane words with the word EXPLICATIVE.
1. Remove all punctuation.
1. Remove all (known) Unicode characters.
1. Remove all whitespace.
1. Replace ordered numbers, e.g., 1st, with the word form, e.g., first.
1. Replace abbreviations and contractions with the expanded form.
1. Convert all words to lowercase.

Following this tokenization procedure, we found all of the n-grams (n=1,2,3,4) and tallied the total count of each type of n-gram.  The following table shows the total counts of n-grams:

``` {r NGramSummary, results='asis', cache=TRUE}
kable(GramSummary, format = 'markdown')
```

Additionally, we have the following charts that show which n-grams are most frequent:

``` {r NGramCharts, cache=TRUE}
# Plot the top twenty for each n-gram
WordsToPlot <- as.data.frame(tokenCombined$uniqueWords[1:20,])
TwoGramsToPlot <- as.data.frame(tokenCombined$uniqueTwoGrams[1:20,])
ThreeGramsToPlot <- as.data.frame(tokenCombined$uniqueThreeGrams[1:20,])
FourGramsToPlot <- as.data.frame(tokenCombined$uniqueFourGrams[1:20,])

WordPlot <- ggplot(data=WordsToPlot, aes(x=words,y=Freq))
WordPlot <- WordPlot + geom_bar(stat="identity", fill = "blue") + coord_flip()
WordPlot <- WordPlot + xlab("Word") + ylab("count")
WordPlot <- WordPlot + ggtitle("Top Twenty Most Frequent Words")
WordPlot <- WordPlot + theme(axis.text.x = element_text(angle=270, hjust=1, vjust=0.5))
print(WordPlot)
 
TwoGramPlot <- ggplot(TwoGramsToPlot, aes(x=TwoGrams,y=Freq))
TwoGramPlot <- TwoGramPlot + geom_bar(stat="identity", fill="red") + coord_flip()
TwoGramPlot <- TwoGramPlot + xlab("2-Gram") + ylab("count")
TwoGramPlot <- TwoGramPlot + ggtitle("Top Twenty Most Frequent 2-Grams")
TwoGramPlot <- TwoGramPlot + theme(axis.text.x = element_text(angle=270, hjust=1, vjust=0.5))
print(TwoGramPlot)

ThreeGramPlot <- ggplot(ThreeGramsToPlot, aes(x=ThreeGrams,y=Freq))
ThreeGramPlot <- ThreeGramPlot + geom_bar(stat="identity", fill="green") + coord_flip()
ThreeGramPlot <- ThreeGramPlot + xlab("3-Gram") + ylab("count")
ThreeGramPlot <- ThreeGramPlot + ggtitle("Top Twenty Most Frequent 3-Grams")
ThreeGramPlot <- ThreeGramPlot + theme(axis.text.x = element_text(angle=270, hjust=1, vjust=0.5))
print(ThreeGramPlot)

FourGramPlot <- ggplot(FourGramsToPlot, aes(x=FourGrams,y=Freq))
FourGramPlot <- FourGramPlot + geom_bar(stat="identity", fill="black") + coord_flip()
FourGramPlot <- FourGramPlot + xlab("4-Gram") + ylab("count")
FourGramPlot <- FourGramPlot + ggtitle("Top Twenty Most Frequent 4-Grams")
FourGramPlot <- FourGramPlot + theme(axis.text.x = element_text(angle=270, hjust=1, vjust=0.5))
print(FourGramPlot)
```

From these charts, we see that the higher the order of the n-gram, the flatter the distribution, i.e., the peak count decreases as n increases.

# Prediction Modeling
To develop the language prediction model, we will develop a Markov chain model which will use n-grams to predict the next word in the sequence.  From the sampled data above, we will be able to compute the probability of each word and n-gram (up to n=4).  However, we will need to compress the data so that the size of the model becomes useable.  Some techniques for data compression are

- Stemming,
- Removing stop words, and
- Decreasing the sample size.

Each of these techniques will be investigated to determine their effectiveness for data compression and prediction accuracy.  For the probability model, we will most likely use Katz' back-off model with Good-Turing smoothing and cross-validation, which will drive the Markov prediction algorithm.